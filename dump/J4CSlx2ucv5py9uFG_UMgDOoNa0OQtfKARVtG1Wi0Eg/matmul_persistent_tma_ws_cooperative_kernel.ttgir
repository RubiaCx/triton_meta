#loc = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0)
#mma = #triton_gpu.nvidia_mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], instrShape = [16, 64, 16]}>
#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], hasLeadingOffset = true}>
#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0], hasLeadingOffset = false}>
module attributes {"triton_gpu.num-ctas" = 1 : i32, "triton_gpu.num-warp-groups-per-cta" = 2 : i32, "triton_gpu.num-warps" = 4 : i32, triton_gpu.target = "cuda:90", "triton_gpu.threads-per-warp" = 32 : i32} {
  tt.func public @matmul_persistent_tma_ws_cooperative_kernel(%arg0: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0), %arg1: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0), %arg2: !tt.ptr<i8, 0> {tt.nv_tma_desc = 1 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0), %arg3: i32 {tt.divisibility = 16 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0), %arg4: i32 {tt.divisibility = 16 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0), %arg5: i32 {tt.divisibility = 16 : i32} loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":120:0)) attributes {noinline = false} {
    %c-2_i32 = arith.constant -2 : i32 loc(#loc1)
    %c4_i32 = arith.constant 4 : i32 loc(#loc1)
    %c3_i64 = arith.constant 3 : i64 loc(#loc1)
    %c1_i64 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 1 : i64 loc(#loc1)
    %true = arith.constant {async_task_id = dense<0> : vector<1xi32>} true loc(#loc1)
    %c3_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 3 : i32 loc(#loc1)
    %cst = arith.constant {async_task_id = dense<0> : vector<1xi32>} dense<0.000000e+00> : tensor<64x64xf32, #mma> loc(#loc1)
    %c128_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 128 : i32 loc(#loc1)
    %c127_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 127 : i32 loc(#loc1)
    %c64_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 64 : i32 loc(#loc1)
    %c8_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 8 : i32 loc(#loc1)
    %c63_i32 = arith.constant {async_task_id = dense<0> : vector<1xi32>} 63 : i32 loc(#loc1)
    %c2_i32 = arith.constant 2 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %0 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x128x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %1 = triton_gpu.local_alloc  : () -> !tt.memdesc<3x64x128xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
    %2 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %3 = triton_gpu.memdesc_subview %2[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %3, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %4 = triton_gpu.memdesc_subview %2[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %4, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %5 = triton_gpu.memdesc_subview %2[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %5, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %6 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %7 = triton_gpu.memdesc_subview %6[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %7, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %8 = triton_gpu.memdesc_subview %6[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %8, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %9 = triton_gpu.memdesc_subview %6[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %9, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %10 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %11 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %12 = triton_gpu.memdesc_subview %10[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %12, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %13 = triton_gpu.memdesc_subview %11[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %13, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %14 = triton_gpu.memdesc_subview %10[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %14, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %15 = triton_gpu.memdesc_subview %11[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %15, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %16 = triton_gpu.memdesc_subview %10[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %16, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %17 = triton_gpu.memdesc_subview %11[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %17, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %18 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %19 = triton_gpu.local_alloc  : () -> !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %20 = triton_gpu.memdesc_subview %18[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %20, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %21 = triton_gpu.memdesc_subview %19[%c0_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %21, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %22 = triton_gpu.memdesc_subview %18[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %22, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %23 = triton_gpu.memdesc_subview %19[%c1_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %23, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %24 = triton_gpu.memdesc_subview %18[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %24, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    %25 = triton_gpu.memdesc_subview %19[%c2_i32] : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    triton_nvidia_gpu.init_barrier %25, 1 : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
    gpu.barrier loc(#loc)
    %26 = triton_nvidia_gpu.get_canonical_warp_id : i32 loc(#loc)
    %27 = arith.divui %26, %c4_i32 : i32 loc(#loc)
    %28 = arith.cmpi eq, %27, %c0_i32 : i32 loc(#loc)
    scf.if %28 {
      triton_nvidia_gpu.reg_dealloc 40 loc(#loc)
      %30 = arith.addi %arg3, %c63_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc30)
      %31 = arith.divsi %30, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc31)
      %32 = arith.addi %arg4, %c63_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc32)
      %33 = arith.divsi %32, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc33)
      %34 = arith.muli %31, %33 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc6)
      %35 = tt.get_program_id x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc7)
      %36 = tt.get_num_programs x {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc8)
      %37 = arith.muli %33, %c8_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc9)
      %38 = arith.addi %arg5, %c127_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc34)
      %39 = arith.divsi %38, %c128_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc35)
      scf.for %arg6 = %35 to %34 step %36  : i32 {
        %40 = arith.divsi %arg6, %37 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc12)
        %41 = arith.muli %40, %c8_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc13)
        %42 = arith.subi %31, %41 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc14)
        %43 = arith.minsi %42, %c8_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc15)
        %44 = arith.remsi %arg6, %37 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc16)
        %45 = arith.remsi %44, %43 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc17)
        %46 = arith.addi %41, %45 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc18)
        %47 = arith.divsi %44, %43 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc19)
        %48 = arith.muli %46, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc20)
        %49 = arith.muli %47, %c64_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc21)
        %50 = arith.addi %39, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %51 = arith.extsi %50 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc22)
        %52 = arith.subi %51, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %53 = arith.subi %arg6, %35 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %54 = arith.divui %53, %36 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
        %55 = arith.extsi %54 {async_task_id = dense<0> : vector<1xi32>} : i32 to i64 loc(#loc22)
        %56 = arith.muli %55, %52 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %57 = arith.divui %56, %c3_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %58 = arith.muli %57, %c3_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %59 = arith.subi %56, %58 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %60 = arith.trunci %59 {async_task_id = dense<0> : vector<1xi32>} : i64 to i32 loc(#loc22)
        %61 = arith.andi %57, %c1_i64 {async_task_id = dense<0> : vector<1xi32>} : i64 loc(#loc22)
        %62 = arith.trunci %61 {async_task_id = dense<0> : vector<1xi32>} : i64 to i1 loc(#loc22)
        %63:3 = scf.for %arg7 = %c0_i32 to %39 step %c1_i32 iter_args(%arg8 = %c0_i32, %arg9 = %62, %arg10 = %60) -> (i32, i1, i32)  : i32 {
          %64 = triton_gpu.memdesc_subview %19[%arg10] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc23)
          %65 = arith.xori %arg9, %true : i1 loc(#loc23)
          %66 = arith.extui %65 : i1 to i32 loc(#loc23)
          triton_nvidia_gpu.wait_barrier %64, %66 {async_task_id = dense<0> : vector<1xi32>} : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc23)
          %67 = triton_gpu.memdesc_subview %2[%arg10] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.barrier_expect %67, 32768 {async_task_id = dense<0> : vector<1xi32>}, %true : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %68 = triton_gpu.memdesc_subview %1[%arg10, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3x64x128xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x128xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.async_tma_copy_global_to_local %arg0[%48, %arg8] %68, %67, %true {async_task_id = dense<0> : vector<1xi32>} : <i8, 0>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <64x128xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %69 = triton_gpu.memdesc_subview %0[%arg10, %c0_i32, %c0_i32] {async_task_id = dense<0> : vector<1xi32>} : !tt.memdesc<3x128x64xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          triton_nvidia_gpu.async_tma_copy_global_to_local %arg1[%arg8, %49] %69, %67, %true {async_task_id = dense<0> : vector<1xi32>} : <i8, 0>, <1xi64, #shared1, #triton_gpu.shared_memory, mutable> -> <128x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc)
          %70 = arith.addi %arg8, %c128_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc24)
          %71 = arith.addi %arg10, %c1_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %72 = arith.cmpi uge, %71, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %73 = arith.cmpi ult, %71, %c3_i32 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %74 = arith.addi %arg10, %c-2_i32 : i32 loc(#loc22)
          %75 = arith.select %72, %74, %71 {async_task_id = dense<0> : vector<1xi32>} : i32 loc(#loc22)
          %76 = arith.xori %arg9, %true {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %77 = arith.andi %72, %76 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %78 = arith.andi %73, %arg9 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          %79 = arith.ori %77, %78 {async_task_id = dense<0> : vector<1xi32>} : i1 loc(#loc22)
          scf.yield {async_task_id = dense<0> : vector<1xi32>} %70, %79, %75 : i32, i1, i32 loc(#loc25)
        } {async_task_id = dense<0> : vector<1xi32>} loc(#loc22)
      } {async_task_id = dense<0> : vector<1xi32>} loc(#loc11)
    } {async_task_id = dense<0> : vector<1xi32>} loc(#loc)
    %29 = arith.cmpi eq, %27, %c1_i32 : i32 loc(#loc)
    scf.if %29 {
      triton_nvidia_gpu.reg_alloc 232 loc(#loc)
      %30 = arith.addi %arg3, %c63_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc30)
      %31 = arith.divsi %30, %c64_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc31)
      %32 = arith.addi %arg4, %c63_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc32)
      %33 = arith.divsi %32, %c64_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc33)
      %34 = arith.muli %31, %33 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc6)
      %35 = tt.get_program_id x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc7)
      %36 = tt.get_num_programs x {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc8)
      %37 = arith.muli %33, %c8_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc9)
      %38 = arith.addi %arg5, %c127_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc34)
      %39 = arith.divsi %38, %c128_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc35)
      %40 = triton_gpu.local_alloc  {async_task_id = dense<1> : vector<1xi32>} : () -> !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc26)
      scf.for %arg6 = %35 to %34 step %36  : i32 {
        %41 = arith.divsi %arg6, %37 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc12)
        %42 = arith.muli %41, %c8_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc13)
        %43 = arith.subi %31, %42 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc14)
        %44 = arith.minsi %43, %c8_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc15)
        %45 = arith.remsi %arg6, %37 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc16)
        %46 = arith.remsi %45, %44 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc17)
        %47 = arith.addi %42, %46 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc18)
        %48 = arith.divsi %45, %44 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc19)
        %49 = arith.muli %47, %c64_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc20)
        %50 = arith.muli %48, %c64_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc21)
        %51 = arith.addi %39, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %52 = arith.extsi %51 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc22)
        %53 = arith.subi %52, %c1_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %54 = arith.subi %arg6, %35 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %55 = arith.divui %54, %36 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
        %56 = arith.extsi %55 {async_task_id = dense<1> : vector<1xi32>} : i32 to i64 loc(#loc22)
        %57 = arith.muli %56, %53 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %58 = arith.divui %57, %c3_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %59 = arith.muli %58, %c3_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %60 = arith.subi %57, %59 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %61 = arith.trunci %60 {async_task_id = dense<1> : vector<1xi32>} : i64 to i32 loc(#loc22)
        %62 = arith.andi %58, %c1_i64 {async_task_id = dense<1> : vector<1xi32>} : i64 loc(#loc22)
        %63 = arith.trunci %62 {async_task_id = dense<1> : vector<1xi32>} : i64 to i1 loc(#loc22)
        %64:3 = scf.for %arg7 = %c0_i32 to %39 step %c1_i32 iter_args(%arg8 = %cst, %arg9 = %63, %arg10 = %61) -> (tensor<64x64xf32, #mma>, i1, i32)  : i32 {
          %66 = triton_gpu.memdesc_subview %2[%arg10] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %67 = arith.extsi %arg9 {async_task_id = dense<1> : vector<1xi32>} : i1 to i32 loc(#loc)
          triton_nvidia_gpu.wait_barrier %66, %67 {async_task_id = dense<1> : vector<1xi32>} : <1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc)
          %68 = triton_gpu.memdesc_subview %1[%arg10, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3x64x128xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<64x128xf16, #shared, #triton_gpu.shared_memory> loc(#loc)
          %69 = triton_gpu.memdesc_subview %0[%arg10, %c0_i32, %c0_i32] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3x128x64xf16, #shared, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<128x64xf16, #shared, #triton_gpu.shared_memory> loc(#loc)
          %70 = triton_nvidia_gpu.warp_group_dot %68, %69, %arg8 {async_task_id = dense<1> : vector<1xi32>, inputPrecision = 0 : i32} : !tt.memdesc<64x128xf16, #shared, #triton_gpu.shared_memory> * !tt.memdesc<128x64xf16, #shared, #triton_gpu.shared_memory> -> tensor<64x64xf32, #mma> loc(#loc27)
          %71 = triton_gpu.memdesc_subview %19[%arg10] {async_task_id = dense<1> : vector<1xi32>} : !tt.memdesc<3xi64, #shared1, #triton_gpu.shared_memory, mutable> -> !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable> loc(#loc27)
          %thread_id_x = gpu.thread_id  x loc(#loc27)
          %72 = builtin.unrealized_conversion_cast %thread_id_x : index to i32 loc(#loc27)
          %73 = arith.remui %72, %c128_i32 : i32 loc(#loc27)
          %74 = arith.divui %73, %c8_i32 : i32 loc(#loc27)
          %75 = arith.divui %74, %c4_i32 : i32 loc(#loc27)
          %76 = arith.remui %74, %c4_i32 : i32 loc(#loc27)
          %77 = arith.xori %76, %75 : i32 loc(#loc27)
          %78 = arith.muli %77, %c4_i32 : i32 loc(#loc27)
          %79 = arith.addi %78, %76 : i32 loc(#loc27)
          %80 = arith.remui %73, %c8_i32 : i32 loc(#loc27)
          %81 = arith.cmpi eq, %80, %c0_i32 : i32 loc(#loc27)
          %82 = arith.cmpi ult, %79, %c1_i32 : i32 loc(#loc27)
          %83 = arith.andi %81, %82 : i1 loc(#loc27)
          triton_nvidia_gpu.mbarrier_arrive %71, %83, %79 {async_task_id = dense<1> : vector<1xi32>, operandSegmentSizes = array<i32: 1, 1, 1>, trackAsyncOp = false} : !tt.memdesc<1xi64, #shared1, #triton_gpu.shared_memory, mutable>, i1, i32 loc(#loc27)
          %84 = arith.addi %arg10, %c1_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %85 = arith.cmpi uge, %84, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %86 = arith.cmpi ult, %84, %c3_i32 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %87 = arith.addi %arg10, %c-2_i32 : i32 loc(#loc22)
          %88 = arith.select %85, %87, %84 {async_task_id = dense<1> : vector<1xi32>} : i32 loc(#loc22)
          %89 = arith.xori %arg9, %true {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %90 = arith.andi %85, %89 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %91 = arith.andi %86, %arg9 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          %92 = arith.ori %90, %91 {async_task_id = dense<1> : vector<1xi32>} : i1 loc(#loc22)
          scf.yield {async_task_id = dense<1> : vector<1xi32>} %70, %92, %88 : tensor<64x64xf32, #mma>, i1, i32 loc(#loc25)
        } {async_task_id = dense<1> : vector<1xi32>} loc(#loc22)
        %65 = arith.truncf %64#0 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x64xf32, #mma> to tensor<64x64xf16, #mma> loc(#loc28)
        triton_nvidia_gpu.async_tma_store_wait {async_task_id = dense<1> : vector<1xi32>, pendings = 0 : i32} loc(#loc26)
        triton_gpu.local_store %65, %40 {async_task_id = dense<1> : vector<1xi32>} : tensor<64x64xf16, #mma> -> !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc26)
        triton_nvidia_gpu.fence_async_shared {async_task_id = dense<1> : vector<1xi32>, bCluster = false} loc(#loc26)
        triton_nvidia_gpu.async_tma_copy_local_to_global %arg2[%49, %50] %40 {async_task_id = dense<1> : vector<1xi32>} : <i8, 0>, <64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc26)
      } {async_task_id = dense<1> : vector<1xi32>} loc(#loc11)
      triton_nvidia_gpu.async_tma_store_wait {pendings = 0 : i32} loc(#loc11)
      triton_gpu.local_dealloc %40 : !tt.memdesc<64x64xf16, #shared, #triton_gpu.shared_memory, mutable> loc(#loc11)
    } {async_task_id = dense<1> : vector<1xi32>} loc(#loc)
    tt.return loc(#loc29)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/home/hoy/triton-fb/python/triton/language/standard.py":40:22)
#loc3 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":137:27)
#loc4 = loc("/home/hoy/triton-fb/python/triton/language/standard.py":40:28)
#loc5 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":137:54)
#loc6 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":137:43)
#loc7 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":138:35)
#loc8 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":138:66)
#loc9 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":141:42)
#loc10 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":160:37)
#loc11 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":138:50)
#loc12 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":142:26)
#loc13 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":143:33)
#loc14 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":144:39)
#loc15 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":144:52)
#loc16 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":145:38)
#loc17 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":145:58)
#loc18 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":145:31)
#loc19 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":146:44)
#loc20 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":155:26)
#loc21 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":156:26)
#loc22 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":160:26)
#loc23 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":166:20)
#loc24 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":173:22)
#loc25 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":173:12)
#loc26 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":178:56)
#loc27 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":172:37)
#loc28 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":175:27)
#loc29 = loc("/home/hoy/triton-fb/python/tutorials/10-warp-specialized-matmul.py":138:4)
#loc30 = loc(callsite(#loc2 at #loc3))
#loc31 = loc(callsite(#loc4 at #loc3))
#loc32 = loc(callsite(#loc2 at #loc5))
#loc33 = loc(callsite(#loc4 at #loc5))
#loc34 = loc(callsite(#loc2 at #loc10))
#loc35 = loc(callsite(#loc4 at #loc10))
